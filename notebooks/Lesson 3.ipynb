{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Linking spark from Python, Scala, Java\n",
    "\n",
    "\n",
    "###python \n",
    "run bin/spark_submit\n",
    "import pyspark \n",
    "\n",
    "\n",
    "\n",
    "##Calculating PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#: src/main/scala/SparkPi.scala \n",
    "/** Import the spark and math packages */\n",
    "import scala.math.random\n",
    "import org.apache.spark._\n",
    "/** Computes an approximation to pi */\n",
    "object SparkPi {\n",
    "def main(args: Array[String]) {\n",
    "/** Create the SparkConf object */\n",
    "val conf = new SparkConf().setAppName(\"Spark Pi\")\n",
    "/** Create the SparkContext */\n",
    "val spark = new SparkContext(conf)\n",
    "/** business logic to calculate Pi */\n",
    "val slices = if (args.length > 0) args(0).toInt else 2\n",
    "val n = math.min(100000L * slices, Int.MaxValue).toInt  // avoid overflow\n",
    "val count = spark.parallelize(1 until n, slices).map { i =>\n",
    "val x = random * 2 - 1\n",
    "val y = random * 2 - 1\n",
    "if (x*x + y*y < 1) 1 else 0\n",
    "}.reduce(_ + _)\n",
    "/** Printing the value of Pi */\n",
    "println(\"Pi is roughly \" + 4.0 * count / n)\n",
    "/** Stop the SparkContext */\n",
    "spark.stop()\n",
    "}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bash-4.1# cat sparkpi.sbt \n",
    "name := \"SparkPi Project\"\n",
    "version := \"1.0\"\n",
    "scalaVersion := \"2.10.4\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.3.1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$SPARK_HOME/bin/spark-submit \\\n",
    "--class \"SparkPi\" \\\n",
    "--master local[4] \\\n",
    "target/scala-2.10/sparkpi-project_2.10-1.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##spark application: weather data -> hottest dates with some precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlContext.implicits._\n",
    "\n",
    "case class Weather(date: String, temp: Int, precipitation: Double)\n",
    "\n",
    "val weather = sc.textFile(\"input/tmp/labdata/sparkdata/nycweather.csv\").map(_.split(\",\")).map(w => Weather(w(0), w(1).trim.toInt,w(2).trim.toDouble)).toDF()\n",
    "\n",
    "weather.registerTempTable(\"weather\")\n",
    "\n",
    "val hottest_with_precip = sqlContext.sql(\"SELECT * FROM weather WHERE precipitation > 0.0 ORDER BY temp DESC\")\n",
    "\n",
    "hottest_with_precip.map(x => (\"Date: \" + x(0), \"Temp : \" + x(1),\"Precip: \" + x(2))).top(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Spark application with MLlib\n",
    "\n",
    "Clustering drop-off points for three taxi clusters\n",
    "Best where to get a taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val taxiFile = sc.textFile(\"input/tmp/labdata/sparkdata/nyctaxisub.csv\")\n",
    "\n",
    "taxiFile.count()\n",
    "\n",
    "val taxiData=taxiFile.filter(_.contains(\"2013\")).filter(_.split(\",\")(3)!=\"\").filter(_.split(\",\")(4)!=\"\")\n",
    "\n",
    "\n",
    "taxiData.count()\n",
    "\n",
    "\n",
    "val taxiFence=taxiData.filter(_.split(\",\")(3).toDouble>40.70).\n",
    " filter(_.split(\",\")(3).toDouble<40.86).\n",
    "filter(_.split(\",\")(4).toDouble>(-74.02)).\n",
    "filter(_.split(\",\")(4).toDouble<(-73.93))\n",
    "\n",
    "taxiFence.count()\n",
    "\n",
    "\n",
    "val\n",
    "taxi=taxiFence.map{line=>Vectors.dense(line.split(',').slice(3,5).map(_\n",
    ".toDouble))}\n",
    "\n",
    "val iterationCount=10\n",
    "val clusterCount=3\n",
    "val model=KMeans.train(taxi,clusterCount,iterationCount)\n",
    "val clusterCenters=model.clusterCenters.map(_.toArray)\n",
    "val cost=model.computeCost(taxi)\n",
    "clusterCenters.foreach(lines=>println(lines(0),lines(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Spark streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.log4j.Logger\n",
    "import org.apache.log4j.Level\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "Logger.getLogger(\"akka\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.StreamingContext._\n",
    "\n",
    "\n",
    "val ssc = new StreamingContext(sc,Seconds(1))\n",
    "\n",
    "val lines = ssc.socketTextStream(\"localhost\",7777)\n",
    "\n",
    "val pass = lines.map(_.split(\",\")).\n",
    "map(pass=>(pass(15),pass(7).toInt)).\n",
    "reduceByKey(_+_)\n",
    "\n",
    "pass.print()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
