{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RDD\n",
    "creates a DAG (called transformation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###fault-tolerant \n",
    "(DAGs can be replayed on nodes that went offline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Transformations and actions\n",
    "Transformation return pointer to RDD\n",
    "Actions return value come from action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Creating RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parallelize existing RDD (lazy evaluation: but won't create RDD yet. This happens when actions are invoked on it)\n",
    "- reference a dataset (e.g. from hdfs, cassandra, amazon s3)\n",
    "- transforming existing RDD (e.g. filter out strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of creating a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start spark-shell\n",
    "# create some data\n",
    "val data = 1 to 1000\n",
    "#Parallelize that data\n",
    "val distData = sc.parallelize(data)\n",
    "# Perform additional transformations or invoke action on it\n",
    "distData.filter(...)\n",
    "#Alternatively, create RDD from external dataset\n",
    "val readmeFile = sc.textFile(\"Readme.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MapReduce word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "val WordCounts = textFile.flatMap(line => line.split(\" \"))\n",
    " .map(word => (word,1))\n",
    " .reduceByKey((a,b) => a+b)\n",
    "\n",
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View DAG of RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linesLength.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###How an action is executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "<console>:1: error: ';' expected but '.' found.",
      "       from IPython.display import Image",
      "                   ^",
      "<console>:2: error: unclosed character literal",
      "       Image(filename='../images/Spark_action_execution.png', width=500)",
      "                      ^",
      "<console>:2: error: unclosed character literal",
      "       Image(filename='../images/Spark_action_execution.png', width=500)",
      "                                                           ^"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='../images/Spark_action_execution.png', width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Transformation\n",
    "- map(func)\n",
    "- filter(func)\n",
    "- flatMap(func)\n",
    "- join(func)\n",
    "- reduceByKey(func)\n",
    "- sortByKey(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Action\n",
    "\n",
    "- collect()\n",
    "- count()\n",
    "- first()\n",
    "- take(n)\n",
    "- foreach(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##RDD persistence\n",
    "Speed gains through persisting/caching the data on each node.\n",
    "Speeds up subsequent access (10x faster)\n",
    "\n",
    "###Two methods RDD persistence\n",
    "- persist() --> disk, memory, many more\n",
    "- cache() --> persist(using MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##shared variables and key-value pairs\n",
    "\n",
    "Broadcast variables\n",
    "- read-only\n",
    "- distribute broadcast variables using eff. broadcast algorithm\n",
    "\n",
    "Accumulators\n",
    "- impl. counters and sums\n",
    "- only driver can read acc. values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##PairRDDFunctions\n",
    "work on pairs of RDDs.\n",
    "\n",
    "Example\n",
    "reduceByKey((a,b) => a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
