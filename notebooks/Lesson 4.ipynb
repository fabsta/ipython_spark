{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Spark libraries\n",
    "\n",
    "\n",
    "##GraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SparkSQL\n",
    "###queries in \n",
    "- SQL\n",
    "- HiveQL (against hive)\n",
    "- Scala\n",
    "\n",
    "###SchemaRDD\n",
    "- row object\n",
    "- schema\n",
    "\n",
    "\n",
    "###Context\n",
    "create sql context from spark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Inferring a schema using reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class Person(name: String, age: Int)\n",
    "#arguments are names of column\n",
    "\n",
    "#create RDD of Person project\n",
    "val people = sc.textFile(\"example...\")\n",
    "    .map(_.split(\",\"))\n",
    "    .map(p=> Person(p(0),p(1).trim.toInt)\n",
    "    \n",
    "#register RDD as a table    \n",
    "people.registerTempTable(\"people\")\n",
    "\n",
    "#run sql statement\n",
    "val teenagers = sqlContext.sql(\"select name from people where age >= 13 and age <= 19\")\n",
    "\n",
    "teenagers.map(t => \"Name: \"+t(0)).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SparkStreaming\n",
    "##receives from\n",
    "- Kafka\n",
    "- Flume\n",
    "- HDFS\n",
    "- Twitter\n",
    "\n",
    "##pushes into\n",
    "- HDFS\n",
    "- Databases\n",
    "- Dashboar\n",
    "\n",
    "##DStream\n",
    "sequence of RDD\n",
    "\n",
    "\n",
    "##Sliding window operations\n",
    "windowed computations (every 30 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of words coming from the TCP socket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.StreamingContext._\n",
    "\n",
    "#Create StreamingContext\n",
    "val conf = new\n",
    "SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\")\n",
    "val ssc = new StreamingContext(conf,Seconds[1])\n",
    "\n",
    "#Create DStream\n",
    "val lines = ssc.socketTextStream(\"localhost\",9999)\n",
    "\n",
    "#split into words\n",
    "lines.flatMap(_.split(\" \"))\n",
    "\n",
    "#count words\n",
    "val pairs = words.map(word => (word,1))\n",
    "val wordCounts = pairs.reduceByKey(_ + _)\n",
    "\n",
    "# print to console\n",
    "wordCounts.print()\n",
    "\n",
    "\n",
    "# start the computation by\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "To run it\n",
    "\n",
    "./bin/run-example streaming.NetworkCount localhost 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MLib\n",
    "\n",
    "- classification\n",
    "- regression\n",
    "- clustering\n",
    "- collaborative filtering\n",
    "- dimenionality reduction\n",
    "\n",
    "##GraphX\n",
    "- social networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
